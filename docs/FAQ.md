# Frequently Asked Questions (FAQ)

## General Questions

### What is NTMM?
NTMM (NorthernTribe Medical Models) is a knowledge distillation pipeline for creating efficient medical reasoning models. It trains a large teacher model on medical datasets, then distills that knowledge into a smaller, faster student model.

### Who owns the NTMM models?
All NTMM student models generated by this pipeline are owned by NorthernTribe Research and licensed under MIT.

### Can I use NTMM for commercial purposes?
Yes, NTMM is licensed under MIT, which allows commercial use. However, ensure you comply with the licenses of any datasets you use for training.

## Installation & Setup

### What Python version do I need?
Python 3.10 or higher is required.

### Do I need a GPU?
No, but it's highly recommended. Training on CPU will be significantly slower. The Qwen models (1.5B teacher, 0.5B student) are designed to run on consumer GPUs.

### How much disk space do I need?
- Models: ~3-5GB (teacher + student)
- Datasets: ~500MB-2GB (depends on configuration)
- Total recommended: 10GB free space

### Installation fails with "torch not found"
Install PyTorch first:
```bash
# For CUDA 12.1
pip install torch --index-url https://download.pytorch.org/whl/cu121

# For CPU only
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

## Training & Data

### How long does training take?
- Quick mode (`./run_all_steps.sh quick`): 5-15 minutes on GPU
- Full training: 1-4 hours depending on dataset size and hardware

### Can I use my own datasets?
Yes! Either:
1. Place CSV files with `text` and `label` columns in `data/`
2. Add your dataset adapter to `src/dataset_adapters.py`

### What medical datasets are supported?
See `src/dataset_adapters.py` for the full list. Currently includes:
- NorthernTribe-Research/comprehensive-healthbench-v2
- TimSchopf/medical_abstracts
- BI55/MedText
- eswardivi/medical_qa
- chenhaodev/medmcqa_instruct

### How do I add a new dataset?
Add an entry to `DATASET_REGISTRY` in `src/dataset_adapters.py`:
```python
"your-dataset-name": {
    "text_columns": ["text_field"],
    "label_column": "label_field",
    "splits": ["train"],
}
```

### Can I change the number of classes?
Yes, edit `num_classes` in `mcp.json` under both `teacher_model` and `student_model`.

## Model Configuration

### How do I change the teacher/student models?
Edit `mcp.json`:
```json
"teacher_model": {
    "name": "Qwen/Qwen2-1.5B",  // Change to any HF model
    "num_classes": 5
}
```

### What's the difference between temperature and alpha?
- **Temperature**: Controls softness of probability distributions (higher = softer)
- **Alpha**: Balances distillation loss vs hard label loss (0.5 = equal weight)

### How do I tune hyperparameters?
Edit `training_params` and `distillation_params` in `mcp.json`. Key parameters:
- Learning rates: Start with 2e-5, adjust if loss doesn't decrease
- Batch size: Increase if you have more GPU memory
- Epochs: More epochs = better fit, but watch for overfitting
- Temperature: 2-5 works well for most cases
- Alpha: 0.3-0.7 range is typical

## Deployment

### How do I use the trained model?
See `examples/inference_example.py`:
```bash
python examples/inference_example.py --text "Your medical text"
```

### Can I deploy to Hugging Face Hub?
Yes:
```bash
pip install huggingface_hub
huggingface-cli login
huggingface-cli upload NorthernTribe-Research/ntmm-v1 saved_models/ntmm-student/
```

### How do I integrate NTMM into my application?
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("saved_models/ntmm-student")
tokenizer = AutoTokenizer.from_pretrained("saved_models/ntmm-student")

# Your inference code here
```

### What's the model size and inference speed?
- NTMM student (Qwen2-0.5B): ~500MB, ~10-50ms per inference on GPU
- Suitable for real-time applications

## Troubleshooting

### "CUDA out of memory" error
Reduce batch size in `mcp.json`:
```json
"training_params": {
    "teacher_batch_size": 4,  // Reduce from 8
    "student_batch_size": 8   // Reduce from 16
}
```

### Tests fail with "datasets not found"
Some tests require optional dependencies. Install with:
```bash
pip install -e ".[dev]"
```

### Model performance is poor
Try:
1. Increase training epochs
2. Use more training data
3. Adjust learning rate
4. Check data quality and label distribution
5. Increase teacher model size

### "Teacher model not found" error
Run the pipeline in order:
```bash
python src/prepare_data.py
python src/train_teacher.py
python src/distil_student.py
```

## Best Practices

### How much data do I need?
- Minimum: 1,000 samples per class
- Recommended: 5,000+ samples per class
- More data = better performance

### Should I balance my classes?
Yes, imbalanced classes can hurt performance. Consider:
- Oversampling minority classes
- Using class weights
- Collecting more data for underrepresented classes

### How do I evaluate model quality?
1. Check accuracy and F1 scores in `evaluation_report.json`
2. Review per-class metrics
3. Test on held-out data
4. Validate with domain experts

## Contributing

### How can I contribute?
See [CONTRIBUTING.md](../CONTRIBUTING.md) for guidelines.

### I found a bug, what should I do?
Open an issue on GitHub with:
- Description of the bug
- Steps to reproduce
- Your environment (OS, Python version, etc.)
- Error messages

## Legal & Ethics

### Is NTMM suitable for clinical use?
No. NTMM is a research tool and should not be used for clinical decision-making without proper validation, regulatory approval, and expert oversight.

### What about patient privacy?
Never train on patient data without proper authorization, de-identification, and compliance with regulations (HIPAA, GDPR, etc.).

### Can I publish research using NTMM?
Yes! Please cite the project (see [CITATION.cff](../CITATION.cff)).

## Still have questions?
Open an issue on GitHub or check the [documentation](../README.md).
