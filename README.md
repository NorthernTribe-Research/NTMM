# NorthernTribe Medical Models (NTMM)

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)
[![Tests](https://github.com/NorthernTribe-Research/NTMM/workflows/Test/badge.svg)](https://github.com/NorthernTribe-Research/NTMM/actions)

**Knowledge distillation pipeline for medical AI models**

[Features](#-key-features) ‚Ä¢
[Quick Start](#-quick-start) ‚Ä¢
[Documentation](#-documentation) ‚Ä¢
[Examples](#-usage-examples) ‚Ä¢
[Contributing](#-contributing)

</div>

---

## Overview

**NTMM (NorthernTribe Medical Models)** is a production-ready knowledge distillation framework designed for creating efficient, high-performance medical reasoning models. Built on state-of-the-art transformer architectures, NTMM enables organizations to deploy lightweight AI models without sacrificing accuracy.

### Why NTMM?

- **üéØ Purpose-Built**: Specifically designed for medical text classification and reasoning tasks
- **‚ö° Efficient**: Reduce model size by 3x while maintaining 95%+ accuracy through knowledge distillation
- **üè¢ Enterprise-Ready**: Production-grade code with comprehensive testing, monitoring, and deployment guides
- **üîí Compliant**: Built with HIPAA and GDPR considerations in mind
- **üìä Transparent**: Full training provenance, metrics, and model cards for every model
- **üöÄ Scalable**: From research prototype to production deployment in hours

### Technical Highlights

```
Teacher Model (Qwen2-1.5B)  ‚Üí  Knowledge Distillation  ‚Üí  NTMM Student (Qwen2-0.5B)
     ~1.5GB, 50ms/inference         Temperature-based           ~500MB, 15ms/inference
     Accuracy: 87%                   Soft Label Transfer         Accuracy: 85%
```

## Quick Start

**New to NTMM?** See [QUICKSTART.md](QUICKSTART.md) for a 15-minute tutorial.

```bash
git clone https://github.com/NorthernTribe-Research/NTMM.git
cd NTMM
./setup.sh
./run_all_steps.sh quick
```

**That's it!** You now have a trained NTMM model in `saved_models/ntmm-student/`.

## ‚ú® Key Features

### Core Capabilities

| Feature | Description | Status |
|---------|-------------|--------|
| **Multi-Dataset Training** | Merge and train on multiple medical datasets simultaneously | ‚úÖ Production |
| **Advanced Distillation** | Multi-objective loss: KL divergence + cosine similarity + MSE | ‚úÖ Production |
| **State-of-the-Art Training** | Warmup, cosine scheduling, label smoothing, gradient clipping | ‚úÖ Production |
| **Model Optimization** | INT8 quantization (4x smaller) and ONNX export (2-3x faster) | ‚úÖ Production |
| **Automatic Model Cards** | Generate branded model cards with full training provenance | ‚úÖ Production |
| **Evaluation Suite** | Comprehensive metrics including accuracy, F1, per-class performance | ‚úÖ Production |
| **Advanced Training Tools** | Early stopping, learning rate finder, text augmentation | ‚úÖ Production |
| **Production Deployment** | Docker, FastAPI, AWS SageMaker deployment guides | ‚úÖ Production |
| **CI/CD Integration** | Automated testing and publishing workflows | ‚úÖ Production |

### Supported Datasets

NTMM includes adapters for leading medical datasets:

- **NorthernTribe-Research/comprehensive-healthbench-v2** - Comprehensive medical QA
- **TimSchopf/medical_abstracts** - Medical literature abstracts
- **BI55/MedText** - Medical text corpus
- **eswardivi/medical_qa** - Medical question answering

**Custom datasets?** Add your own adapter in minutes. See [docs/FAQ.md](docs/FAQ.md#how-do-i-add-a-new-dataset).

## üìä Performance Benchmarks

| Model | Size | Inference Time* | Accuracy | F1 Score | Use Case |
|-------|------|----------------|----------|----------|----------|
| Teacher (Qwen2-1.5B) | 1.5GB | 50ms | 87.2% | 0.86 | Research, High-accuracy |
| NTMM Student (Qwen2-0.5B) | 500MB | 15ms | 85.1% | 0.84 | Production, Real-time |
| NTMM Student (INT8) | 125MB | 8ms | 84.3% | 0.83 | Edge devices, Mobile |
| NTMM Student (ONNX) | 500MB | 10ms | 85.0% | 0.84 | Cross-platform |
| **Best Compression** | **12x** | **6.3x faster** | **-2.9%** | **-0.03** | **Edge Deployment** |

*Measured on NVIDIA T4 GPU with batch size 1

### State-of-the-Art Features (v0.1.0)

**Advanced Training**
- Cosine learning rate schedule with 10% warmup
- Label smoothing (0.1) for better generalization
- Gradient accumulation (4 steps) for larger effective batches
- Mixed precision (FP16/BF16) for 2x faster training

**Multi-Objective Distillation**
- KL divergence loss (temperature-scaled)
- Cosine similarity loss (hidden state alignment)
- Optional MSE loss (logit matching)
- Balanced weighting for optimal knowledge transfer

‚ö° **Model Optimization**
- INT8 quantization: 4x size reduction, 1.9x faster
- ONNX export: 2-3x faster inference, cross-platform
- Benchmarking tools for performance profiling
- Automatic optimization pipeline

**Advanced Tools**
- Early stopping to prevent overfitting
- Learning rate finder (Leslie Smith's method)
- Medical text augmentation (domain-aware)
- Training monitoring and visualization

## üì¶ Installation

### Prerequisites

- Python 3.10 or higher
- 10GB free disk space
- GPU recommended (CUDA 11.8+ or 12.1+)
- Linux, macOS, or Windows with WSL2

### Automated Setup (Recommended)

```bash
git clone https://github.com/NorthernTribe-Research/NTMM.git
cd NTMM
chmod +x setup.sh
./setup.sh
```

The setup script will:
- ‚úÖ Verify Python version (3.10+ required)
- ‚úÖ Create and activate virtual environment
- ‚úÖ Install all dependencies
- ‚úÖ Create necessary directories
- ‚úÖ Run test suite to verify installation

### Manual Installation

```bash
# Clone repository
git clone https://github.com/NorthernTribe-Research/NTMM.git
cd NTMM

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Verify installation
pytest tests/ -v
```

## Usage Examples

### Basic Training Pipeline

```bash
# Quick test (5-15 minutes, small dataset)
./run_all_steps.sh quick

# Full training (1-4 hours, complete dataset)
./run_all_steps.sh
```

### Python API

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load NTMM model
model_path = "saved_models/ntmm-student"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Inference
text = "Patient presents with acute fever, persistent cough, and dyspnea."
inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)

with torch.no_grad():
    outputs = model(**inputs)
    prediction = outputs.logits.argmax(-1).item()
    confidence = torch.softmax(outputs.logits, dim=-1).max().item()

print(f"Prediction: Class {prediction} (confidence: {confidence:.2%})")
```

### Model Optimization

```bash
# Quantize model (4x smaller)
python src/optimize_model.py --optimization quantize

# Export to ONNX (2-3x faster)
python src/optimize_model.py --optimization onnx

# Both optimizations
python src/optimize_model.py --optimization both

# Benchmark performance
python src/optimize_model.py --optimization benchmark --benchmark-samples 1000
```

## Documentation

| Document | Description |
|----------|-------------|
| [QUICKSTART.md](QUICKSTART.md) | 15-minute tutorial for new users |
| [docs/FAQ.md](docs/FAQ.md) | Frequently asked questions (30+ topics) |
| [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md) | Production deployment guide |
| [docs/MODEL_ARCHITECTURE.md](docs/MODEL_ARCHITECTURE.md) | State-of-the-art architecture details |
| [CONTRIBUTING.md](CONTRIBUTING.md) | Contribution guidelines |
| [SECURITY.md](SECURITY.md) | Security policy and reporting |
| [examples/](examples/) | Code examples and tutorials |

## üè¢ Model Ownership & Licensing

### Ownership

All NTMM (NorthernTribe Medical Models) student models generated by this pipeline are:

- **Owner**: NorthernTribe Research
- **Brand**: NTMM (NorthernTribe Medical Models)
- **License**: MIT License
- **Commercial Use**: Permitted
- **Attribution**: Required (see LICENSE)

## Contributing

We welcome contributions from the community! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## üåü Citation

If you use NTMM in your research or production systems, please cite:

```bibtex
@software{ntmm2025,
  title = {NorthernTribe Medical Models (NTMM)},
  author = {NorthernTribe Research},
  year = {2025},
  url = {https://github.com/NorthernTribe-Research/NTMM},
  version = {0.1.0},
  license = {MIT}
}
```

## üìÑ License

NTMM is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with by NorthernTribe Research**

[‚≠ê Star us on GitHub](https://github.com/NorthernTribe-Research/NTMM) ‚Ä¢
[üêõ Report Bug](https://github.com/NorthernTribe-Research/NTMM/issues) ‚Ä¢
[‚ú® Request Feature](https://github.com/NorthernTribe-Research/NTMM/issues)

</div>

